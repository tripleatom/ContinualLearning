{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Windows\\.conda\\envs\\ms10\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from spikegadget2nwb.read_spikegadget import get_ephys_folder\n",
    "import spikeinterface.extractors as se\n",
    "import os\n",
    "import numpy as np\n",
    "import spikeinterface.preprocessing as spre\n",
    "import time\n",
    "import spikeinterface as si\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='border:1px solid #ddd; padding:10px;'><strong>NwbRecordingExtractor: 128 channels - 30.0kHz - 1 segments - 30,473,680 samples - 1,015.79s (16.93 minutes) - int16 dtype - 7.27 GiB</strong></div><details style='margin-left: 10px;'>  <summary><strong>Channel IDs</strong></summary><ul>[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
       "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
       "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
       "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
       "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
       "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
       " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
       " 126 127] </details><details style='margin-left: 10px;'>  <summary><strong>Annotations</strong></summary><ul><li> <strong> is_filtered </strong>: False</li></ul> </details><details style='margin-left: 10px;'><summary><strong>Channel Properties</strong></summary><ul><details><summary> <strong> gain_to_uV </strong> </summary>[0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195\n",
       " 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195\n",
       " 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195\n",
       " 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195\n",
       " 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195\n",
       " 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195\n",
       " 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195\n",
       " 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195\n",
       " 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195\n",
       " 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195\n",
       " 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195]</details><details><summary> <strong> offset_to_uV </strong> </summary>[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
       " 0. 0. 0. 0. 0. 0. 0. 0.]</details><details><summary> <strong> location </strong> </summary>[[ 325.   75.]\n",
       " [ 350.  225.]\n",
       " [ 350.  175.]\n",
       " [ 325.  250.]\n",
       " [ 350.  150.]\n",
       " [ 325.  225.]\n",
       " [ 325.  275.]\n",
       " [ 325.  100.]\n",
       " [ 350.  350.]\n",
       " [ 325.  175.]\n",
       " [ 325.  150.]\n",
       " [ 350.  375.]\n",
       " [ 350.  100.]\n",
       " [ 350.  250.]\n",
       " [ 350.   50.]\n",
       " [ 325.  375.]\n",
       " [ 350.  125.]\n",
       " [ 350.  300.]\n",
       " [ 350.   75.]\n",
       " [ 325.  350.]\n",
       " [ 325.  325.]\n",
       " [ 350.   25.]\n",
       " [ 350.  325.]\n",
       " [   0.  250.]\n",
       " [   0.   50.]\n",
       " [ 350.    0.]\n",
       " [   0.    0.]\n",
       " [ 325.  300.]\n",
       " [   0.   25.]\n",
       " [ 350.  275.]\n",
       " [  25.  225.]\n",
       " [ 350.  200.]\n",
       " [ 325.  125.]\n",
       " [ 325.  200.]\n",
       " [ 325.    0.]\n",
       " [ 325.   25.]\n",
       " [  25.  200.]\n",
       " [   0.  200.]\n",
       " [ 325.   50.]\n",
       " [  25.  175.]\n",
       " [   0.  225.]\n",
       " [   0.  175.]\n",
       " [  25.  375.]\n",
       " [  25.  150.]\n",
       " [   0.  375.]\n",
       " [   0.  150.]\n",
       " [  25.  350.]\n",
       " [  25.  125.]\n",
       " [  25.  100.]\n",
       " [   0.  350.]\n",
       " [  25.   75.]\n",
       " [  25.  325.]\n",
       " [  25.   50.]\n",
       " [   0.  325.]\n",
       " [  25.   25.]\n",
       " [  25.  300.]\n",
       " [  25.    0.]\n",
       " [   0.  300.]\n",
       " [   0.  125.]\n",
       " [  25.  275.]\n",
       " [   0.  100.]\n",
       " [   0.  275.]\n",
       " [   0.   75.]\n",
       " [  25.  250.]\n",
       " [1000.  175.]\n",
       " [ 975.  350.]\n",
       " [ 975.  150.]\n",
       " [1000.  325.]\n",
       " [1000.  125.]\n",
       " [ 975.  325.]\n",
       " [1000.  100.]\n",
       " [1000.  300.]\n",
       " [1000.   75.]\n",
       " [ 975.  300.]\n",
       " [1000.   50.]\n",
       " [1000.  275.]\n",
       " [1000.   25.]\n",
       " [ 975.  275.]\n",
       " [1000.    0.]\n",
       " [1000.  250.]\n",
       " [ 975.  250.]\n",
       " [ 975.  125.]\n",
       " [1000.  225.]\n",
       " [ 975.  100.]\n",
       " [ 975.  225.]\n",
       " [ 975.   75.]\n",
       " [1000.  200.]\n",
       " [ 975.   50.]\n",
       " [ 975.  200.]\n",
       " [ 650.  225.]\n",
       " [ 975.   25.]\n",
       " [ 975.    0.]\n",
       " [ 650.  200.]\n",
       " [ 675.  200.]\n",
       " [ 650.   25.]\n",
       " [ 650.    0.]\n",
       " [ 675.  325.]\n",
       " [1000.  150.]\n",
       " [ 675.   75.]\n",
       " [1000.  375.]\n",
       " [ 675.  100.]\n",
       " [ 975.  375.]\n",
       " [ 650.  350.]\n",
       " [1000.  350.]\n",
       " [ 975.  175.]\n",
       " [ 675.  150.]\n",
       " [ 675.  350.]\n",
       " [ 675.  125.]\n",
       " [ 650.  175.]\n",
       " [ 675.  375.]\n",
       " [ 650.  150.]\n",
       " [ 675.  300.]\n",
       " [ 675.   50.]\n",
       " [ 650.  375.]\n",
       " [ 650.  125.]\n",
       " [ 650.  325.]\n",
       " [ 675.   25.]\n",
       " [ 650.  300.]\n",
       " [ 650.  275.]\n",
       " [ 675.  175.]\n",
       " [ 650.  250.]\n",
       " [ 675.    0.]\n",
       " [ 650.   50.]\n",
       " [ 675.  275.]\n",
       " [ 650.  100.]\n",
       " [ 675.  250.]\n",
       " [ 650.   75.]\n",
       " [ 675.  225.]]</details><details><summary> <strong> group </strong> </summary>['shank1' 'shank1' 'shank1' 'shank1' 'shank1' 'shank1' 'shank1' 'shank1'\n",
       " 'shank1' 'shank1' 'shank1' 'shank1' 'shank1' 'shank1' 'shank1' 'shank1'\n",
       " 'shank1' 'shank1' 'shank1' 'shank1' 'shank1' 'shank1' 'shank1' 'shank0'\n",
       " 'shank0' 'shank1' 'shank0' 'shank1' 'shank0' 'shank1' 'shank0' 'shank1'\n",
       " 'shank1' 'shank1' 'shank1' 'shank1' 'shank0' 'shank0' 'shank1' 'shank0'\n",
       " 'shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0'\n",
       " 'shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0'\n",
       " 'shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0'\n",
       " 'shank3' 'shank3' 'shank3' 'shank3' 'shank3' 'shank3' 'shank3' 'shank3'\n",
       " 'shank3' 'shank3' 'shank3' 'shank3' 'shank3' 'shank3' 'shank3' 'shank3'\n",
       " 'shank3' 'shank3' 'shank3' 'shank3' 'shank3' 'shank3' 'shank3' 'shank3'\n",
       " 'shank3' 'shank2' 'shank3' 'shank3' 'shank2' 'shank2' 'shank2' 'shank2'\n",
       " 'shank2' 'shank3' 'shank2' 'shank3' 'shank2' 'shank3' 'shank2' 'shank3'\n",
       " 'shank3' 'shank2' 'shank2' 'shank2' 'shank2' 'shank2' 'shank2' 'shank2'\n",
       " 'shank2' 'shank2' 'shank2' 'shank2' 'shank2' 'shank2' 'shank2' 'shank2'\n",
       " 'shank2' 'shank2' 'shank2' 'shank2' 'shank2' 'shank2' 'shank2' 'shank2']</details><details><summary> <strong> brain_area </strong> </summary>['V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1'\n",
       " 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1'\n",
       " 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1'\n",
       " 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1'\n",
       " 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1'\n",
       " 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1'\n",
       " 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1'\n",
       " 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1'\n",
       " 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1'\n",
       " 'V1' 'V1']</details><details><summary> <strong> label </strong> </summary>['shank1elec0' 'shank1elec1' 'shank1elec2' 'shank1elec3' 'shank1elec4'\n",
       " 'shank1elec5' 'shank1elec6' 'shank1elec7' 'shank1elec8' 'shank1elec9'\n",
       " 'shank1elec10' 'shank1elec11' 'shank1elec12' 'shank1elec13'\n",
       " 'shank1elec14' 'shank1elec15' 'shank1elec16' 'shank1elec17'\n",
       " 'shank1elec18' 'shank1elec19' 'shank1elec20' 'shank1elec21'\n",
       " 'shank1elec22' 'shank0elec23' 'shank0elec24' 'shank1elec25'\n",
       " 'shank0elec26' 'shank1elec27' 'shank0elec28' 'shank1elec29'\n",
       " 'shank0elec30' 'shank1elec31' 'shank1elec32' 'shank1elec33'\n",
       " 'shank1elec34' 'shank1elec35' 'shank0elec36' 'shank0elec37'\n",
       " 'shank1elec38' 'shank0elec39' 'shank0elec40' 'shank0elec41'\n",
       " 'shank0elec42' 'shank0elec43' 'shank0elec44' 'shank0elec45'\n",
       " 'shank0elec46' 'shank0elec47' 'shank0elec48' 'shank0elec49'\n",
       " 'shank0elec50' 'shank0elec51' 'shank0elec52' 'shank0elec53'\n",
       " 'shank0elec54' 'shank0elec55' 'shank0elec56' 'shank0elec57'\n",
       " 'shank0elec58' 'shank0elec59' 'shank0elec60' 'shank0elec61'\n",
       " 'shank0elec62' 'shank0elec63' 'shank3elec64' 'shank3elec65'\n",
       " 'shank3elec66' 'shank3elec67' 'shank3elec68' 'shank3elec69'\n",
       " 'shank3elec70' 'shank3elec71' 'shank3elec72' 'shank3elec73'\n",
       " 'shank3elec74' 'shank3elec75' 'shank3elec76' 'shank3elec77'\n",
       " 'shank3elec78' 'shank3elec79' 'shank3elec80' 'shank3elec81'\n",
       " 'shank3elec82' 'shank3elec83' 'shank3elec84' 'shank3elec85'\n",
       " 'shank3elec86' 'shank3elec87' 'shank3elec88' 'shank2elec89'\n",
       " 'shank3elec90' 'shank3elec91' 'shank2elec92' 'shank2elec93'\n",
       " 'shank2elec94' 'shank2elec95' 'shank2elec96' 'shank3elec97'\n",
       " 'shank2elec98' 'shank3elec99' 'shank2elec100' 'shank3elec101'\n",
       " 'shank2elec102' 'shank3elec103' 'shank3elec104' 'shank2elec105'\n",
       " 'shank2elec106' 'shank2elec107' 'shank2elec108' 'shank2elec109'\n",
       " 'shank2elec110' 'shank2elec111' 'shank2elec112' 'shank2elec113'\n",
       " 'shank2elec114' 'shank2elec115' 'shank2elec116' 'shank2elec117'\n",
       " 'shank2elec118' 'shank2elec119' 'shank2elec120' 'shank2elec121'\n",
       " 'shank2elec122' 'shank2elec123' 'shank2elec124' 'shank2elec125'\n",
       " 'shank2elec126' 'shank2elec127']</details></ul></details>"
      ],
      "text/plain": [
       "NwbRecordingExtractor: 128 channels - 30.0kHz - 1 segments - 30,473,680 samples \n",
       "                       1,015.79s (16.93 minutes) - int16 dtype - 7.27 GiB\n",
       "  file_path: D:\\cl\\rf_reconstruction\\head_fixed\\CnL14_20241004_153555.rec\\CnL14_20241004_153555.rec.nwb"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subject_id = \"CnL14\"\n",
    "exp_date = \"20241004\"\n",
    "exp_time = \"153555\"\n",
    "session_description = subject_id + '_' + exp_date + '_' + exp_time + '.rec'\n",
    "ephys_folder = Path(r\"D:\\cl\\rf_reconstruction\\head_fixed\")\n",
    "# ephys_folder = Path(r\"D:\\cl\\rf_reconstruction\\freelymoving\")\n",
    "# ephys_folder = get_ephys_folder()\n",
    "folder = ephys_folder / session_description\n",
    "\n",
    "nwb_file = folder / (session_description + '.nwb')\n",
    "rec = se.NwbRecordingExtractor(nwb_file)\n",
    "rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bad_ch_id(rec, folder, load_if_exists=True):\n",
    "    if load_if_exists and os.path.exists(folder / 'bad_ch_id.npy'):\n",
    "        bad_ch_id = np.load(folder / 'bad_ch_id.npy')\n",
    "    else:\n",
    "        bad_ch_id, _ = spre.detect_bad_channels(\n",
    "            rec, num_random_chunks=400, n_neighbors=5, dead_channel_threshold=-0.2)\n",
    "\n",
    "        np.save(folder / 'bad_ch_id.npy', bad_ch_id)\n",
    "\n",
    "    print('Bad channel IDs:', bad_ch_id)\n",
    "    return bad_ch_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad channel IDs: [  0   2   3   8  11  16  20  24  27  36  39  40  43  47  48  59  63  67\n",
      "  83  87 103 111 115 118 119 121 122 125 126 127]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([  1,   4,   5,   6,   7,   9,  10,  12,  13,  14,  15,  17,  18,\n",
       "        19,  21,  22,  23,  25,  26,  28,  29,  30,  31,  32,  33,  34,\n",
       "        35,  37,  38,  41,  42,  44,  45,  46,  49,  50,  51,  52,  53,\n",
       "        54,  55,  56,  57,  58,  60,  61,  62,  64,  65,  66,  68,  69,\n",
       "        70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,\n",
       "        84,  85,  86,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "        98,  99, 100, 101, 102, 104, 105, 106, 107, 108, 109, 110, 112,\n",
       "       113, 114, 116, 117, 120, 123, 124])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_filtered = spre.bandpass_filter(rec, freq_min=300, freq_max=6000)\n",
    "bad_ch_id = get_bad_ch_id(rec_filtered, folder)\n",
    "remaining_ch = np.array([ch for ch in rec.get_channel_ids() if ch not in bad_ch_id])\n",
    "\n",
    "# export remaining channel ids to a npy file\n",
    "np.save(os.path.join(folder, 'remaining_ch.npy'), remaining_ch)\n",
    "remaining_ch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 7\n",
    "chunk_size = 900\n",
    "n_timepoints = rec_filtered.get_num_frames()\n",
    "n_channels = rec_filtered.get_num_channels()\n",
    "num_chunks = int(np.ceil(n_timepoints / chunk_size))\n",
    "\n",
    "# load artifact indices if exists\n",
    "if os.path.exists(folder / 'artifact_indices.npy'):\n",
    "    artifact_indices = np.load(folder / 'artifact_indices.npy')\n",
    "else:\n",
    "# mask artifacts\n",
    "    norms = np.zeros((num_chunks, n_channels))\n",
    "    for i in range(num_chunks):\n",
    "        start = int(i * chunk_size)\n",
    "        end = int(np.minimum((i + 1) * chunk_size, n_timepoints))\n",
    "        chunk = rec_filtered.get_traces(start_frame=start, end_frame=end, return_scaled=True)\n",
    "\n",
    "        norms[i] = np.linalg.norm(chunk, axis=0)\n",
    "\n",
    "    \n",
    "    use_it = np.ones(num_chunks, dtype=bool)\n",
    "# if detect artifacts in a chunk, don't use it and the two neighboring chunks\n",
    "\n",
    "    for m in range(n_channels):\n",
    "        if m in bad_ch_id:\n",
    "            continue\n",
    "        vals = norms[:, m]\n",
    "\n",
    "        sigma0 = np.std(vals)\n",
    "        mean0 = np.mean(vals)\n",
    "\n",
    "        artifact_indices = np.where(vals > mean0 + threshold * sigma0)[0]\n",
    "\n",
    "        # check if the first chunk is above threshold, ensure that we don't use negative indices later\n",
    "        negIndBool = np.where(artifact_indices > 0)[0]\n",
    "\n",
    "        # check if the last chunk is above threshold to avoid a IndexError\n",
    "        maxIndBool = np.where(artifact_indices < num_chunks - 1)[0]\n",
    "\n",
    "        use_it[artifact_indices] = 0\n",
    "        use_it[artifact_indices[negIndBool] - 1] = 0  # don't use the neighbor chunks either\n",
    "        use_it[artifact_indices[maxIndBool] + 1] = 0  # don't use the neighbor chunks either\n",
    "\n",
    "        print(\"For channel %d: mean=%.2f, stdev=%.2f, chunk size = %d, n_artifacts = %d\" % (m, mean0, sigma0, chunk_size, len(artifact_indices)))\n",
    "\n",
    "\n",
    "    artifact_indices = np.where(use_it == 0)[0]\n",
    "    artifact_indices = artifact_indices * chunk_size\n",
    "    # save artifact indices\n",
    "    np.save(folder / 'artifact_indices.npy', artifact_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_time = chunk_size / rec.get_sampling_frequency()*1000\n",
    "\n",
    "if artifact_indices.size > 0:\n",
    "    rec_rm_artifacts = spre.remove_artifacts(rec_filtered, list_triggers=artifact_indices, ms_before=0, ms_after=chunk_time)\n",
    "\n",
    "else:\n",
    "    rec_rm_artifacts = rec_filtered\n",
    "\n",
    "\n",
    "rec_clean = rec_rm_artifacts.channel_slice(remaining_ch)\n",
    "rec_ref = spre.common_reference(rec_clean, reference='global', operator='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_shank0 = np.where(rec_ref.get_channel_groups()=='shank0')\n",
    "sh0_ch = rec_ref.get_channel_ids()[idx_shank0]\n",
    "rec_sh0 = rec_ref.channel_slice(sh0_ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style='border:1px solid #ddd; padding:10px;'><strong>ChannelSliceRecording: 23 channels - 30.0kHz - 1 segments - 30,473,680 samples - 1,015.79s (16.93 minutes) - int16 dtype - 1.31 GiB</strong></div><details style='margin-left: 10px;'>  <summary><strong>Channel IDs</strong></summary><ul>[23 26 28 30 37 41 42 44 45 46 49 50 51 52 53 54 55 56 57 58 60 61 62] </details><details style='margin-left: 10px;'>  <summary><strong>Annotations</strong></summary><ul><li> <strong> is_filtered </strong>: True</li></ul> </details><details style='margin-left: 10px;'><summary><strong>Channel Properties</strong></summary><ul><details><summary> <strong> gain_to_uV </strong> </summary>[0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195\n",
       " 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195 0.195]</details><details><summary> <strong> offset_to_uV </strong> </summary>[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]</details><details><summary> <strong> location </strong> </summary>[[  0. 250.]\n",
       " [  0.   0.]\n",
       " [  0.  25.]\n",
       " [ 25. 225.]\n",
       " [  0. 200.]\n",
       " [  0. 175.]\n",
       " [ 25. 375.]\n",
       " [  0. 375.]\n",
       " [  0. 150.]\n",
       " [ 25. 350.]\n",
       " [  0. 350.]\n",
       " [ 25.  75.]\n",
       " [ 25. 325.]\n",
       " [ 25.  50.]\n",
       " [  0. 325.]\n",
       " [ 25.  25.]\n",
       " [ 25. 300.]\n",
       " [ 25.   0.]\n",
       " [  0. 300.]\n",
       " [  0. 125.]\n",
       " [  0. 100.]\n",
       " [  0. 275.]\n",
       " [  0.  75.]]</details><details><summary> <strong> group </strong> </summary>['shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0'\n",
       " 'shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0'\n",
       " 'shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0' 'shank0']</details><details><summary> <strong> brain_area </strong> </summary>['V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1'\n",
       " 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1' 'V1']</details><details><summary> <strong> label </strong> </summary>['shank0elec23' 'shank0elec26' 'shank0elec28' 'shank0elec30'\n",
       " 'shank0elec37' 'shank0elec41' 'shank0elec42' 'shank0elec44'\n",
       " 'shank0elec45' 'shank0elec46' 'shank0elec49' 'shank0elec50'\n",
       " 'shank0elec51' 'shank0elec52' 'shank0elec53' 'shank0elec54'\n",
       " 'shank0elec55' 'shank0elec56' 'shank0elec57' 'shank0elec58'\n",
       " 'shank0elec60' 'shank0elec61' 'shank0elec62']</details></ul></details>"
      ],
      "text/plain": [
       "ChannelSliceRecording: 23 channels - 30.0kHz - 1 segments - 30,473,680 samples \n",
       "                       1,015.79s (16.93 minutes) - int16 dtype - 1.31 GiB"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_sh0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 23)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rec_sh0.get_traces(start_frame=0, end_frame=100000).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of channels: 23\n",
      "Number of timepoints: 30473680\n",
      "Sampling frequency: 30000.0 Hz\n",
      "Channel 0: [  0. 250.]\n",
      "Channel 1: [0. 0.]\n",
      "Channel 2: [ 0. 25.]\n",
      "Channel 3: [ 25. 225.]\n",
      "Channel 4: [  0. 200.]\n",
      "Channel 5: [  0. 175.]\n",
      "Channel 6: [ 25. 375.]\n",
      "Channel 7: [  0. 375.]\n",
      "Channel 8: [  0. 150.]\n",
      "Channel 9: [ 25. 350.]\n",
      "Channel 10: [  0. 350.]\n",
      "Channel 11: [25. 75.]\n",
      "Channel 12: [ 25. 325.]\n",
      "Channel 13: [25. 50.]\n",
      "Channel 14: [  0. 325.]\n",
      "Channel 15: [25. 25.]\n",
      "Channel 16: [ 25. 300.]\n",
      "Channel 17: [25.  0.]\n",
      "Channel 18: [  0. 300.]\n",
      "Channel 19: [  0. 125.]\n",
      "Channel 20: [  0. 100.]\n",
      "Channel 21: [  0. 275.]\n",
      "Channel 22: [ 0. 75.]\n",
      "Loading traces\n"
     ]
    }
   ],
   "source": [
    "import mountainsort5 as ms5\n",
    "import json\n",
    "from tempfile import TemporaryDirectory\n",
    "from mountainsort5.util import create_cached_recording\n",
    "\n",
    "experiment_length = rec_sh0.get_duration() / 60  # in minutes\n",
    "recording_whitened = spre.whiten(rec_sh0, dtype='float32')\n",
    "\n",
    "threshold = 5.5\n",
    "phase1_detect_time_radius_msec = .4\n",
    "\n",
    "with TemporaryDirectory() as tmpdir:\n",
    "    # recording_cached = create_cached_recording(recording_whitened, folder=tmpdir)\n",
    "    recording_cached = recording_whitened\n",
    "\n",
    "\n",
    "    if experiment_length < 25:\n",
    "        sorting_params = ms5.Scheme1SortingParameters(\n",
    "            detect_time_radius_msec=phase1_detect_time_radius_msec, detect_threshold=threshold, detect_channel_radius=80,\n",
    "            )\n",
    "        sorting = ms5.sorting_scheme1(\n",
    "            recording_cached, sorting_parameters=sorting_params)\n",
    "        \n",
    "        assert isinstance(sorting, si.BaseSorting)\n",
    "    else:\n",
    "        sorting_params = ms5.Scheme2SortingParameters(\n",
    "            phase1_detect_threshold=threshold, detect_threshold=threshold,\n",
    "            phase1_detect_channel_radius=100, detect_channel_radius=100, phase1_detect_time_radius_msec=phase1_detect_time_radius_msec, training_duration_sec=5*60,\n",
    "            training_recording_sampling_mode='uniform')\n",
    "        sorting = ms5.sorting_scheme2(\n",
    "            recording=recording_whitened, sorting_parameters=sorting_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = time.strftime(\"%Y%m%d_%H%M\", time.localtime())\n",
    "folder_name = 'sorting_results_' + current_time\n",
    "sort_out_folder = folder / folder_name\n",
    "if not os.path.exists(sort_out_folder):\n",
    "    os.makedirs(sort_out_folder)\n",
    "\n",
    "# write a into json file: sorting_params.json\n",
    "with open(sort_out_folder / 'sorting_params.json', 'w') as f:\n",
    "    json.dump(sorting_params.__dict__, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unit number:1\n"
     ]
    }
   ],
   "source": [
    "print(f'unit number:{len(sorting.get_unit_ids())}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Windows\\.conda\\envs\\ms10\\lib\\site-packages\\spikeinterface\\core\\basesorting.py:264: UserWarning: The registered recording will not be persistent on disk, but only available in memory\n",
      "  warnings.warn(\"The registered recording will not be persistent on disk, but only available in memory\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style='border:1px solid #ddd; padding:10px;'><strong>NumpyFolder: 1 units - 1 segments - 30.0kHz</strong></div><details style='margin-left: 10px;'>  <summary><strong>Unit IDs</strong></summary><ul>[1] </details><details style='margin-left: 10px;'>  <summary><strong>Annotations</strong></summary><ul></details><details style='margin-left: 10px;'><summary><strong>Unit Properties</strong></summary><ul></ul></details>"
      ],
      "text/plain": [
       "NumpyFolder: 1 units - 1 segments - 30.0kHz"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorting.register_recording(rec_ref)\n",
    "sorting.save(folder = os.path.join(sort_out_folder, 'sorting'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## export sorting result to phy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "estimate_sparsity:   0%|          | 0/1016 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "estimate_sparsity: 100%|##########| 1016/1016 [00:02<00:00, 359.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SortingAnalyzer: 98 channels - 1 units - 1 segments - memory - sparse - has recording\n",
      "Loaded 0 extensions: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "estimate_templates_with_accumulator: 100%|##########| 1016/1016 [00:36<00:00, 27.84it/s]\n",
      "write_binary_recording: 100%|##########| 1016/1016 [17:19<00:00,  1.02s/it]\n",
      "spike_amplitudes: 100%|##########| 1016/1016 [17:25<00:00,  1.03s/it]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Extension principal_components requires waveforms to be computed first",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m phy_folder\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m     17\u001b[0m     phy_folder\u001b[38;5;241m.\u001b[39mmkdir()\n\u001b[1;32m---> 18\u001b[0m \u001b[43mexport_to_phy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msorting_analyzer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mphy_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43mremove_if_exists\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Windows\\.conda\\envs\\ms10\\lib\\site-packages\\spikeinterface\\exporters\\to_phy.py:226\u001b[0m, in \u001b[0;36mexport_to_phy\u001b[1;34m(sorting_analyzer, output_folder, compute_pc_features, compute_amplitudes, sparsity, copy_binary, remove_if_exists, template_mode, add_quality_metrics, add_template_metrics, additional_properties, dtype, verbose, use_relative_path, **job_kwargs)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compute_pc_features:\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sorting_analyzer\u001b[38;5;241m.\u001b[39mhas_extension(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprincipal_components\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 226\u001b[0m         sorting_analyzer\u001b[38;5;241m.\u001b[39mcompute(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprincipal_components\u001b[39m\u001b[38;5;124m\"\u001b[39m, n_components\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mby_channel_local\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mjob_kwargs)\n\u001b[0;32m    228\u001b[0m     pca_extension \u001b[38;5;241m=\u001b[39m sorting_analyzer\u001b[38;5;241m.\u001b[39mget_extension(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprincipal_components\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    230\u001b[0m     pca_extension\u001b[38;5;241m.\u001b[39mrun_for_all_spikes(output_folder \u001b[38;5;241m/\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpc_features.npy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mjob_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Windows\\.conda\\envs\\ms10\\lib\\site-packages\\spikeinterface\\core\\sortinganalyzer.py:1150\u001b[0m, in \u001b[0;36mSortingAnalyzer.compute\u001b[1;34m(self, input, save, extension_params, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   1103\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;124;03mCompute one extension or several extensiosn.\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;124;03mInternally calls compute_one_extension() or compute_several_extensions() depending on the input type.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1147\u001b[0m \n\u001b[0;32m   1148\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_one_extension(extension_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m, save\u001b[38;5;241m=\u001b[39msave, verbose\u001b[38;5;241m=\u001b[39mverbose, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1151\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m   1152\u001b[0m     params_, job_kwargs \u001b[38;5;241m=\u001b[39m split_job_kwargs(kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Windows\\.conda\\envs\\ms10\\lib\\site-packages\\spikeinterface\\core\\sortinganalyzer.py:1228\u001b[0m, in \u001b[0;36mSortingAnalyzer.compute_one_extension\u001b[1;34m(self, extension_name, save, verbose, **kwargs)\u001b[0m\n\u001b[0;32m   1226\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1227\u001b[0m         ok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_extension(dependency_name) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1228\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m ok, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExtension \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextension_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdependency_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to be computed first\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1230\u001b[0m extension_instance \u001b[38;5;241m=\u001b[39m extension_class(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m   1231\u001b[0m extension_instance\u001b[38;5;241m.\u001b[39mset_params(save\u001b[38;5;241m=\u001b[39msave, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "\u001b[1;31mAssertionError\u001b[0m: Extension principal_components requires waveforms to be computed first"
     ]
    }
   ],
   "source": [
    "from spikeinterface import create_sorting_analyzer\n",
    "from spikeinterface.exporters import export_to_phy\n",
    "\n",
    "\n",
    "sorting_analyzer_folder = sort_out_folder / 'sorting_analyzer' \n",
    "\n",
    "if not os.path.exists(sorting_analyzer_folder):\n",
    "    sorting_analyzer = create_sorting_analyzer(sorting=sorting, recording=rec_ref, format='memory',)\n",
    "    \n",
    "print(sorting_analyzer)\n",
    "sorting_analyzer.compute(\"random_spikes\")\n",
    "# sorting_analyzer.compute(\"waveforms\", ms_before=2.0, ms_after=2.0)\n",
    "sorting_analyzer.compute([\"templates\"])\n",
    "\n",
    "phy_folder = sort_out_folder / 'phy'\n",
    "if not phy_folder.exists():\n",
    "    phy_folder.mkdir()\n",
    "export_to_phy(\n",
    "    sorting_analyzer,\n",
    "    phy_folder,\n",
    "    verbose=True,\n",
    "    remove_if_exists=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Windows\\.conda\\envs\\ms10\\lib\\site-packages\\spikeinterface\\core\\job_tools.py:103: UserWarning: `n_jobs` is not set so parallel processing is disabled! To speed up computations, it is recommended to set n_jobs either globally (with the `spikeinterface.set_global_job_kwargs()` function) or locally (with the `n_jobs` argument). Use `spikeinterface.set_global_job_kwargs?` for more information about job_kwargs.\n",
      "  warnings.warn(\n",
      "write_binary_recording: 100%|##########| 1016/1016 [17:02<00:00,  1.01s/it]\n",
      "extract PCs: 100%|##########| 1016/1016 [43:08<00:00,  2.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:\n",
      "phy template-gui  D:\\cl\\rf_reconstruction\\head_fixed\\CnL14_20241004_153555.rec\\sorting_results_20241021_0816\\phy\\params.py\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert channel position to 2d\n",
    "file_path = r\"D:\\cl\\rf_reconstruction\\head_fixed\\CnL14_20241004_153555.rec\\sorting_results_20241021_0816\\phy\\channel_positions.npy\"\n",
    "channel_positions = np.load(file_path)\n",
    "channel_positions = channel_positions[:, :2]\n",
    "np.save(file_path, channel_positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ms10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
